"""
–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Parquet —Ñ–∞–π–ª–æ–≤.

–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å–æ–±—ã—Ç–∏–π —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤—ã–±–æ—Ä–∫–∏ —Ñ–∞–π–ª–æ–≤
–¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
"""

import pandas as pd
import pyarrow.parquet as pq
from pathlib import Path
from tqdm import tqdm
import glob
import warnings
import logging
from datetime import datetime
from typing import Optional, List, Dict, Tuple

warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)


def load_parquet_without_embeddings(file_path: str) -> Optional[pd.DataFrame]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç Parquet —Ñ–∞–π–ª, –∏—Å–∫–ª—é—á–∞—è –∫–æ–ª–æ–Ω–∫—É 'embedding' –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.
    
    :param file_path: –ü—É—Ç—å –∫ Parquet —Ñ–∞–π–ª—É
    :return: DataFrame –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    """
    try:
        if not Path(file_path).exists():
            logger.warning(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return None
            
        parquet_file = pq.ParquetFile(file_path)
        columns = [col for col in parquet_file.schema_arrow.names if col != 'embedding']
        table = pq.read_table(file_path, columns=columns)
        return table.to_pandas()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}", exc_info=True)
        return None


def _add_temporal_features(df: pd.DataFrame, base_date: datetime = None) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫ DataFrame.
    
    :param df: DataFrame —Å –∫–æ–ª–æ–Ω–∫–æ–π 'timestamp'
    :param base_date: –ë–∞–∑–æ–≤–∞—è –¥–∞—Ç–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2020-01-01)
    :return: DataFrame —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    """
    if base_date is None:
        base_date = datetime(2020, 1, 1)
    
    if 'timestamp' not in df.columns:
        return df
    
    df = df.copy()
    df['datetime'] = base_date + pd.to_timedelta(df['timestamp'], unit='s')
    df['date'] = df['datetime'].dt.date
    df['hour'] = df['datetime'].dt.hour
    df['day_of_week'] = df['datetime'].dt.dayofweek
    return df


def _load_events_from_channel(
    channel: str,
    base_path: str,
    user_filter: Optional[Tuple[str, int]] = None,
    sample_users: Optional[List[int]] = None,
    max_files_per_channel: Optional[int] = None,
    sample_ratio: Optional[int] = None
) -> List[pd.DataFrame]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è –∏–∑ –æ–¥–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞.
    
    :param channel: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞
    :param base_path: –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
    :param user_filter: –ö–æ—Ä—Ç–µ–∂ (–∫–æ–ª–æ–Ω–∫–∞, –∑–Ω–∞—á–µ–Ω–∏–µ) –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –æ–¥–Ω–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    :param sample_users: –°–ø–∏—Å–æ–∫ user_id –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    :param max_files_per_channel: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
    :param sample_ratio: –ó–∞–≥—Ä—É–∂–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π —Ñ–∞–π–ª
    :return: –°–ø–∏—Å–æ–∫ DataFrame —Å —Å–æ–±—ã—Ç–∏—è–º–∏
    """
    events_path = Path(base_path) / channel / "events"
    
    if not events_path.exists():
        logger.warning(f"–ü—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {events_path}")
        return []
    
    event_files = sorted(glob.glob(str(events_path / "*.pq")))
    
    if not event_files:
        logger.warning(f"–§–∞–π–ª—ã —Å–æ–±—ã—Ç–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {events_path}")
        return []
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±–æ—Ä–∫—É —Ñ–∞–π–ª–æ–≤
    if sample_ratio is not None and sample_ratio > 1:
        event_files = event_files[::sample_ratio]
        logger.info(f"  –ö–∞–Ω–∞–ª {channel}: –≤—ã–±–æ—Ä–∫–∞ –∫–∞–∂–¥–æ–≥–æ {sample_ratio}-–≥–æ —Ñ–∞–π–ª–∞")
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
    if max_files_per_channel is not None:
        event_files = event_files[:max_files_per_channel]
        logger.info(f"  –ö–∞–Ω–∞–ª {channel}: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ {max_files_per_channel} —Ñ–∞–π–ª–æ–≤")
    
    logger.info(f"  –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞–Ω–∞–ª–∞ {channel} ({len(event_files)} —Ñ–∞–π–ª–æ–≤)...")
    
    all_events = []
    base_date = datetime(2020, 1, 1)
    
    for file_path in tqdm(event_files, desc=f"  {channel}", leave=False):
        try:
            df = load_parquet_without_embeddings(file_path)
            if df is None or len(df) == 0:
                continue
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
            if user_filter is not None:
                col, value = user_filter
                if col not in df.columns:
                    continue
                df = df[df[col] == value].copy()
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Å–ø–∏—Å–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            if sample_users is not None:
                if 'user_id' not in df.columns:
                    continue
                df = df[df['user_id'].isin(sample_users)].copy()
            
            if len(df) > 0:
                df = _add_temporal_features(df, base_date)
                df['channel'] = channel
                all_events.append(df)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}", exc_info=True)
            continue
    
    return all_events


def load_user_events(
    user_id: int,
    base_path: str = "./t_ecd_data/dataset/small",
    channels: Optional[List[str]] = None,
    max_files_per_channel: Optional[int] = None,
    sample_ratio: Optional[int] = None
) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π.
    
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ user_id —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
    - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤ (max_files_per_channel)
    - –í—ã–±–æ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ (sample_ratio - –∫–∞–∂–¥—ã–π N-–π —Ñ–∞–π–ª)
    
    :param user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    :param base_path: –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
    :param channels: –°–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (None = –≤—Å–µ –∫–∞–Ω–∞–ª—ã)
    :param max_files_per_channel: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –Ω–∞ –∫–∞–Ω–∞–ª (None = –≤—Å–µ)
    :param sample_ratio: –ó–∞–≥—Ä—É–∂–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π —Ñ–∞–π–ª (None = –≤—Å–µ —Ñ–∞–π–ª—ã)
    :return: DataFrame —Å —Å–æ–±—ã—Ç–∏—è–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    if channels is None:
        channels = ['marketplace', 'retail', 'offers']
    
    logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–±—ã—Ç–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}...")
    
    all_events = []
    for channel in channels:
        channel_events = _load_events_from_channel(
            channel=channel,
            base_path=base_path,
            user_filter=('user_id', user_id),
            max_files_per_channel=max_files_per_channel,
            sample_ratio=sample_ratio
        )
        all_events.extend(channel_events)
    
    if all_events:
        logger.info("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π...")
        combined = pd.concat(all_events, ignore_index=True)
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(combined):,} —Å–æ–±—ã—Ç–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
        return combined.sort_values(['user_id', 'datetime'])
    
    logger.warning(f"‚ö†Ô∏è  –°–æ–±—ã—Ç–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    return pd.DataFrame()


def load_all_events(
    base_path: str = "./t_ecd_data/dataset/small",
    channels: Optional[List[str]] = None,
    sample_users: Optional[List[int]] = None,
    max_files_per_channel: Optional[int] = None,
    sample_ratio: Optional[int] = None
) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å–æ–±—ã—Ç–∏—è –∏–∑ –≤—Å–µ—Ö –∫–∞–Ω–∞–ª–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π.
    
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–ø–∏—Å–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π (sample_users)
    - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤ (max_files_per_channel)
    - –í—ã–±–æ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤ (sample_ratio - –∫–∞–∂–¥—ã–π N-–π —Ñ–∞–π–ª)
    
    :param base_path: –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
    :param channels: –°–ø–∏—Å–æ–∫ –∫–∞–Ω–∞–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (None = –≤—Å–µ –∫–∞–Ω–∞–ª—ã)
    :param sample_users: –°–ø–∏—Å–æ–∫ user_id –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (None = –≤—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏)
    :param max_files_per_channel: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –Ω–∞ –∫–∞–Ω–∞–ª (None = –≤—Å–µ)
    :param sample_ratio: –ó–∞–≥—Ä—É–∂–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π —Ñ–∞–π–ª (None = –≤—Å–µ —Ñ–∞–π–ª—ã)
    :return: DataFrame —Å–æ –≤—Å–µ–º–∏ —Å–æ–±—ã—Ç–∏—è–º–∏
    """
    if channels is None:
        channels = ['marketplace', 'retail', 'offers']
    
    logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Å–æ–±—ã—Ç–∏–π...")
    
    all_events = []
    for channel in channels:
        channel_events = _load_events_from_channel(
            channel=channel,
            base_path=base_path,
            sample_users=sample_users,
            max_files_per_channel=max_files_per_channel,
            sample_ratio=sample_ratio
        )
        all_events.extend(channel_events)
    
    if all_events:
        logger.info("üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π...")
        combined = pd.concat(all_events, ignore_index=True)
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(combined):,} —Å–æ–±—ã—Ç–∏–π")
        return combined.sort_values(['user_id', 'datetime'])
    
    logger.warning("‚ö†Ô∏è  –°–æ–±—ã—Ç–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    return pd.DataFrame()


def load_reference_data(base_path: str = "./t_ecd_data/dataset/small") -> Dict[str, pd.DataFrame]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏, —Ç–æ–≤–∞—Ä—ã, –±—Ä–µ–Ω–¥—ã).
    
    :param base_path: –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
    :return: –°–ª–æ–≤–∞—Ä—å —Å DataFrame —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤
    :raises FileNotFoundError: –ï—Å–ª–∏ –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    """
    base_path_obj = Path(base_path)
    if not base_path_obj.exists():
        raise FileNotFoundError(f"–ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {base_path}")
    
    datasets = {}
    logger.info("üìö –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    users_path = base_path_obj / "users.pq"
    if users_path.exists():
        datasets['users'] = load_parquet_without_embeddings(str(users_path))
        if datasets['users'] is not None:
            logger.info(f"  ‚úÖ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏: {len(datasets['users']):,} –∑–∞–ø–∏—Å–µ–π")
        else:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ {users_path}")
    else:
        raise FileNotFoundError(f"–§–∞–π–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {users_path}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±—Ä–µ–Ω–¥—ã
    brands_path = base_path_obj / "brands.pq"
    if brands_path.exists():
        datasets['brands'] = load_parquet_without_embeddings(str(brands_path))
        if datasets['brands'] is not None:
            logger.info(f"  ‚úÖ –ë—Ä–µ–Ω–¥—ã: {len(datasets['brands']):,} –∑–∞–ø–∏—Å–µ–π")
    else:
        logger.warning(f"  ‚ö†Ô∏è  –§–∞–π–ª –±—Ä–µ–Ω–¥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {brands_path}")
        datasets['brands'] = pd.DataFrame()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–≤–∞—Ä—ã –ø–æ –∫–∞–Ω–∞–ª–∞–º
    channels = ['marketplace', 'retail', 'offers']
    items_dict = {}
    
    for channel in channels:
        items_path = base_path_obj / channel / "items.pq"
        if items_path.exists():
            items_df = load_parquet_without_embeddings(str(items_path))
            if items_df is not None:
                datasets[f'{channel}_items'] = items_df
                items_dict[f'{channel}_items'] = items_df
                logger.info(f"  ‚úÖ –¢–æ–≤–∞—Ä—ã {channel}: {len(items_df):,} –∑–∞–ø–∏—Å–µ–π")
        else:
            logger.warning(f"  ‚ö†Ô∏è  –§–∞–π–ª —Ç–æ–≤–∞—Ä–æ–≤ {channel} –Ω–µ –Ω–∞–π–¥–µ–Ω: {items_path}")
            datasets[f'{channel}_items'] = pd.DataFrame()
            items_dict[f'{channel}_items'] = pd.DataFrame()
    
    datasets['items_dict'] = items_dict
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤: {len(datasets)}")
    return datasets

